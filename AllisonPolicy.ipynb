{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/allisonlettiere/Desktop/CS238FinalProject/Project.toml\""
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"AA228FinalProject\")\n",
    "    jenv = joinpath(dirname(@__FILE__()), \".\") # this assumes the notebook is in the same dir\n",
    "    # as the Project.toml file, which should be in top level dir of the project. \n",
    "    # Change accordingly if this is not the case.\n",
    "    Pkg.activate(jenv)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "using AA228FinalProject\n",
    "using POMDPs\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using ParticleFilters\n",
    "using POMDPSimulators\n",
    "using Cairo\n",
    "using Gtk\n",
    "using Random\n",
    "using Printf\n",
    "using StaticArrays\n",
    "import Distributions.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoombaPOMDP{Bumper,Bool}(Bumper(), RoombaMDP{DiscreteRoombaStateSpace,Array{RoombaAct,1}}\n",
       "  v_max: Float64 10.0\n",
       "  om_max: Float64 1.0\n",
       "  dt: Float64 0.5\n",
       "  contact_pen: Float64 -1.0\n",
       "  time_pen: Float64 -0.1\n",
       "  goal_reward: Float64 10.0\n",
       "  stairs_penalty: Float64 -10.0\n",
       "  config: Int64 3\n",
       "  sspace: DiscreteRoombaStateSpace\n",
       "  room: AA228FinalProject.Room\n",
       "  aspace: Array{RoombaAct}((9,))\n",
       "  _amap: Dict{RoombaAct,Int64}\n",
       ")"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor = Bumper()\n",
    "config = 3# 1,2, or 3\n",
    "\n",
    "num_x_pts = 20\n",
    "num_y_pts = 20\n",
    "num_th_pts = 5\n",
    "sspace = DiscreteRoombaStateSpace(num_x_pts,num_y_pts,num_th_pts)\n",
    "aspace = vec(collect(RoombaAct(v, om) for v in [0,1,10], om in [-1,0,1]))\n",
    "\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config, sspace=sspace, aspace=aspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mdp (generic function with 2 methods)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_particles = 1000\n",
    "resampler = BumperResampler(num_particles)\n",
    "\n",
    "spf = SimpleParticleFilter(m, resampler)\n",
    "\n",
    "v_noise_coefficient = 2.0\n",
    "om_noise_coefficient = 0.5\n",
    "\n",
    "belief_updater = RoombaParticleFilter(spf, v_noise_coefficient, om_noise_coefficient);\n",
    "\n",
    "mdp(e::RoombaMDP) = e\n",
    "mdp(e::RoombaPOMDP) = e.mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Point Based Value Iteration Policy Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "discountFactor = 0.9\n",
    "goal_xy = get_goal_xy(m)\n",
    "goal_x, goal_y = goal_xy\n",
    "\n",
    "RW = 5\n",
    "if config == 1\n",
    "    x = 10+RW \n",
    "    y = -RW\n",
    "elseif config== 2\n",
    "    x = -20+RW\n",
    "    y = RW\n",
    "else\n",
    "    x = -20+RW\n",
    "    y = RW\n",
    "end\n",
    "\n",
    "ang_to_goal = atan(goal_y - y, goal_x - x)\n",
    "si = RoombaState(x,y,ang_to_goal,0.)\n",
    "sf = RoombaState(goal_x,goal_y, 1, 0.)\n",
    "\n",
    "if(ang_to_goal <= 0.5 && ang_to_goal >= (2*pi - 1) + 0.5)\n",
    "    ang_to_goal = 0.0\n",
    "elif(ang_to_goal > 0.5 && ang_to_goal <= (2*pi - 1) - 2.14)\n",
    "    ang_to_goal = 1.0\n",
    "else\n",
    "    ang_to_goal = -1.0\n",
    "end\n",
    "\n",
    "action = (10, ang_to_goal)\n",
    "startingReward = reward(m, si, action, sf)\n",
    "firstAlphaVector = []\n",
    "\n",
    "for state in 1:length(POMDPs.states(m))\n",
    "    push!(firstAlphaVector, (1/(1-discountFactor)) * startingReward)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: s not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: s not defined",
      "",
      "Stacktrace:",
      " [1] sumNextStateObservation(::SArray{Tuple{4},Float64,1,4}, ::RoombaAct, ::Dict{Any,Any}, ::Int64) at ./In[234]:9",
      " [2] backupBelief(::ParticleCollection{RoombaState}, ::Dict{Any,Any}) at ./In[266]:27",
      " [3] RandomizedPointBasedUpdate(::Array{Any,1}, ::Dict{Any,Any}) at ./In[237]:8",
      " [4] top-level scope at In[267]:12"
     ]
    }
   ],
   "source": [
    "gamma = Dict()\n",
    "gamma[action] = firstAlphaVector\n",
    "\n",
    "B = []\n",
    "dist = initialstate_distribution(m)\n",
    "b = initialize_belief(belief_updater, dist)\n",
    "\n",
    "for i in 1:1\n",
    "    push!(B, b)\n",
    "    b = resample(resampler, b, belief_updater.spf.rng)\n",
    "end\n",
    "\n",
    "gamma = RandomizedPointBasedUpdate(B, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probabilityVector (generic function with 1 method)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function probabilityVector(m, b)    \n",
    "    probabilitiesB = []\n",
    "    ##if the state is not in the belief space, give probability 0\n",
    "    for i in POMDPs.states(m)\n",
    "        if(i in keys(probdict(b)))\n",
    "            push!(probabilitiesB, probdict(b)[i])\n",
    "        else\n",
    "            push!(probabilitiesB, 0.0)\n",
    "        end\n",
    "    end\n",
    "    return probabilitiesB\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probdict (generic function with 1 method)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function probdict(b::AbstractParticleBelief{S}) where {S}\n",
    "    probs = Dict{S, Float64}()\n",
    "    for (i,p) in enumerate(particles(b))\n",
    "        if haskey(probs, p)\n",
    "            probs[p] += weight(b, i)/weight_sum(b)\n",
    "        else\n",
    "            probs[p] = weight(b, i)/weight_sum(b)\n",
    "        end\n",
    "    end\n",
    "    return probs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedPointBasedUpdate (generic function with 1 method)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function RandomizedPointBasedUpdate(B, gamma)\n",
    "    BP = B\n",
    "    gammaP = Dict()\n",
    "    #println(BP)\n",
    "    while !(isempty(BP))\n",
    "        b = rand(BP)\n",
    "        filter!(e->e != b,BP)\n",
    "        alphaVectors = backupBelief(b, gamma)\n",
    "        for action in aspace\n",
    "            if (action in gammaP)\n",
    "                maxU = 0\n",
    "                oldMaxAlpha = gammaP[action]\n",
    "                oldUtility = oldMaxAlpha'probabilityVector(m, b)\n",
    "                newUtility = alphaVectors[action]'probabilityVector(m, b)\n",
    "                if (newUtility >= oldUtility)\n",
    "                    gammaP[action] = alphaVectors[action]\n",
    "                else\n",
    "                    gammaP[action] = oldMaxAlpha\n",
    "                end\n",
    "            else\n",
    "                gammaP[action] = alphaVectors[action]\n",
    "            end\n",
    "        end  \n",
    "    end\n",
    "    return gammaP\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backupBelief (generic function with 1 method)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backupBelief(b, gamma)\n",
    "    O = [0, 1]\n",
    "    alphaAO = Dict()\n",
    "    alphaVectors = Dict()\n",
    "    actionNumber = 1\n",
    "    for action in aspace\n",
    "        for observation in O\n",
    "            bp = update(belief_updater, b, action, observation)\n",
    "            maxTranspose = 0\n",
    "            for a in keys(gamma)\n",
    "                t = gamma[a]'probabilityVector(m, bp)\n",
    "                if (t > maxTranspose)\n",
    "                    maxTranspose = t\n",
    "                    alphaAO[[action, observation + 1]] = gamma[a]\n",
    "                end\n",
    "            end\n",
    "        end \n",
    "        \n",
    "        s = mean(b)\n",
    "        e, gr, gwn, sr, swn, next_x, next_y, next_th, next_status = collisionStatus(s, action)\n",
    "        sp = RoombaState(next_x, next_y, next_th, next_status)\n",
    "\n",
    "        alpha = Dict()\n",
    "        \n",
    "        stateNumber = 1\n",
    "        for state in POMDPs.states(m)          \n",
    "            alpha[[stateNumber, actionNumber]] = reward(m, s, action, sp) + discountFactor * sumNextStateObservation(s, action, alphaAO, stateNumber)\n",
    "            stateNumber += 1\n",
    "        end\n",
    "        alphaVectors[action] = alpha[[:,actionNumber]]\n",
    "        actionNumber += 1\n",
    "    end\n",
    "    return alphaVectors\n",
    "end     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sumNextStateObservation (generic function with 1 method)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sumNextStateObservation(state, action, alphaAO, stateNumber)\n",
    "    sum = 0\n",
    "    e, gr, gwn, sr, swn, next_x, next_y, next_th, next_status, sp = collisionStatus(state, action)\n",
    "    \n",
    "    ##0 -- does not hit wall (false), 1 -- hits wall (true)\n",
    "    for o in [0, 1]\n",
    "        observationFunction = 0\n",
    "        if o == 0\n",
    "            if(!wall_contact(e, s))\n",
    "                observationFunction = 1\n",
    "                ob = 0\n",
    "            end\n",
    "        elseif o == 1\n",
    "            if(wall_contact(e, s))\n",
    "                observationFunction = 1\n",
    "            end\n",
    "        end\n",
    "        transitionFunction = 1 \n",
    "        sum += observationFunction * transitionFunction * alphaAO[[action, o + 1]][stateNumber]\n",
    "    end\n",
    "    return sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ToEnd <: Policy\n",
    "    ts::Int64 # to track the current time-step.\n",
    "end\n",
    "\n",
    "function POMDPs.action(p::ToEnd, b::ParticleCollection{RoombaState})\n",
    "    maxUtility = 0\n",
    "    actionSelection = 0\n",
    "    for action in gamma\n",
    "        alpha = gamma[action]\n",
    "        transpose = alpha'probabilityVector(m, b)\n",
    "        if (transpose > maxUtility)\n",
    "            maxUtility = transpose\n",
    "            actionSelection = action\n",
    "        end\n",
    "    end\n",
    "    return RoombaAct(actionSelection[1], actionSelection[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runEnvironment (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function runEnvironment()\n",
    "    # first seed the environment\n",
    "    Random.seed!(2)\n",
    "\n",
    "    # reset the policy\n",
    "    p = ToEnd(0) # here, the argument sets the time-steps elapsed to 0\n",
    "\n",
    "    # run the simulation\n",
    "    c = @GtkCanvas()\n",
    "    win = GtkWindow(c, \"Roomba Environment\", 600, 600)\n",
    "    for (t, step) in enumerate(stepthrough(m, p, belief_updater, max_steps=100))\n",
    "        @guarded draw(c) do widget\n",
    "\n",
    "            # the following lines render the room, the particles, and the roomba\n",
    "            ctx = getgc(c)\n",
    "            set_source_rgb(ctx,1,1,1)\n",
    "            paint(ctx)\n",
    "            render(ctx, m, step)\n",
    "\n",
    "            # render some information that can help with debugging\n",
    "            # here, we render the time-step, the state, and the observation\n",
    "            move_to(ctx,300,400)\n",
    "            show_text(ctx, @sprintf(\"t=%d, state=%s, o=%.3f\",t,string(step.s),step.o))\n",
    "        end\n",
    "        show(c)\n",
    "        sleep(0.1) # to slow down the simulation\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "BoundsError",
     "evalue": "BoundsError",
     "output_type": "error",
     "traceback": [
      "BoundsError",
      "",
      "Stacktrace:",
      " [1] getindex at ./number.jl:78 [inlined]",
      " [2] action(::ToEnd, ::ParticleCollection{RoombaState}) at ./In[56]:16",
      " [3] action_info at /Users/allisonlettiere/.julia/packages/POMDPModelTools/eHEjm/src/info.jl:30 [inlined]",
      " [4] iterate(::POMDPSimulators.POMDPSimIterator{(:s, :a, :r, :sp, :t, :i, :ai, :b, :o, :bp, :ui),RoombaPOMDP{Bumper,Bool},ToEnd,RoombaParticleFilter,MersenneTwister,ParticleCollection{RoombaState},RoombaState}, ::Tuple{Int64,RoombaState,ParticleCollection{RoombaState}}) at /Users/allisonlettiere/.julia/packages/POMDPSimulators/xyfJM/src/stepthrough.jl:102",
      " [5] iterate at /Users/allisonlettiere/.julia/packages/POMDPSimulators/xyfJM/src/stepthrough.jl:96 [inlined]",
      " [6] iterate(::Base.Iterators.Enumerate{POMDPSimulators.POMDPSimIterator{(:s, :a, :r, :sp, :t, :i, :ai, :b, :o, :bp, :ui),RoombaPOMDP{Bumper,Bool},ToEnd,RoombaParticleFilter,MersenneTwister,ParticleCollection{RoombaState},RoombaState}}, ::Tuple{Int64}) at ./iterators.jl:139",
      " [7] iterate at ./iterators.jl:138 [inlined]",
      " [8] runEnvironment() at ./In[35]:11",
      " [9] top-level scope at util.jl:156",
      " [10] top-level scope at In[81]:1"
     ]
    }
   ],
   "source": [
    "@time runEnvironment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward (generic function with 2 methods)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reward(m::RoombaPOMDP{Bumper,Bool}, s::RoombaState, a::Tuple{Int64,Float64}, sp::RoombaState)\n",
    "    \n",
    "    # penalty for each timestep elapsed\n",
    "    cum_reward = mdp(m).time_pen\n",
    "\n",
    "    # penalty for bumping into wall (not incurred for consecutive contacts)\n",
    "    previous_wall_contact = wall_contact(m,s)\n",
    "    current_wall_contact = wall_contact(m,sp)\n",
    "    if(!previous_wall_contact && current_wall_contact)\n",
    "        cum_reward += mdp(m).contact_pen\n",
    "    end\n",
    "\n",
    "    # terminal rewards\n",
    "    cum_reward += mdp(m).goal_reward*(sp.status == 1.0)\n",
    "    cum_reward += mdp(m).stairs_penalty*(sp.status == -1.0)\n",
    "\n",
    "    return cum_reward  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update (generic function with 2 methods)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update(up::RoombaParticleFilter, b::ParticleCollection{RoombaState}, a, o)\n",
    "    ps = particles(b)\n",
    "    pm = up.spf._particle_memory\n",
    "    wm = up.spf._weight_memory\n",
    "    resize!(pm, 0)\n",
    "    resize!(wm, 0)\n",
    "    sizehint!(pm, n_particles(b))\n",
    "    sizehint!(wm, n_particles(b))\n",
    "    all_terminal = true\n",
    "    for i in 1:n_particles(b)\n",
    "        s = ps[i]\n",
    "        if !isterminal(up.spf.model, s)\n",
    "            all_terminal = false\n",
    "            # noise added here:\n",
    "            a_pert = a + SVector(up.v_noise_coeff*(rand(up.spf.rng)-0.5), up.om_noise_coeff*(rand(up.spf.rng)-0.5))\n",
    "            sp = generate_s(up.spf.model, s, a_pert, up.spf.rng)\n",
    "            push!(pm, sp)\n",
    "            push!(wm, obs_weight(up.spf.model, s, a_pert, sp, o))\n",
    "        end\n",
    "    end\n",
    "    # if all particles are terminal, return previous belief state\n",
    "    if all_terminal\n",
    "        return b\n",
    "    end\n",
    "\n",
    "    return resample(up.spf.resample, WeightedParticleBelief{RoombaState}(pm, wm, sum(wm), nothing), up.spf.rng)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collisionStatus (generic function with 1 method)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function collisionStatus(s, a)\n",
    "    e = mdp(m)\n",
    "    v, om = action\n",
    "    v = clamp(v, 0.0, e.v_max)\n",
    "    om = clamp(om, -e.om_max, e.om_max)\n",
    "\n",
    "    # propagate dynamics without wall considerations\n",
    "    x, y, th, _ = s\n",
    "    dt = e.dt\n",
    "\n",
    "    # dynamics assume robot rotates and then translates\n",
    "    next_th = wrap_to_pi(th + om*dt)\n",
    "\n",
    "    # make sure we arent going through a wall\n",
    "    p0 = SVector(x, y)\n",
    "    heading = SVector(cos(next_th), sin(next_th))\n",
    "    des_step = v*dt\n",
    "    next_x, next_y = AA228FinalProject.legal_translate(e.room, p0, heading, des_step)\n",
    "\n",
    "    # Determine whether goal state or stairs have been reached\n",
    "    grn = mdp(m).room.goal_rect\n",
    "    gwn = mdp(m).room.goal_wall\n",
    "    srn = mdp(m).room.stair_rect\n",
    "    swn = mdp(m).room.stair_wall\n",
    "    gr = mdp(m).room.rectangles[grn]\n",
    "    sr = mdp(m).room.rectangles[srn]    \n",
    "    \n",
    "    next_status = 1.0*AA228FinalProject.contact_wall(gr, gwn, [next_x, next_y]) - 1.0*AA228FinalProject.contact_wall(sr, swn, [next_x, next_y])\n",
    "    sp = RoombaState(next_x, next_y, next_th, next_status)\n",
    "    \n",
    "    return e, gr, gwn, sr, swn, next_x, next_y, next_th, next_status, sp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
