{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/allisonlettiere/Desktop/CS238FinalProject/Project.toml\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"AA228FinalProject\")\n",
    "    jenv = joinpath(dirname(@__FILE__()), \".\") # this assumes the notebook is in the same dir\n",
    "    # as the Project.toml file, which should be in top level dir of the project. \n",
    "    # Change accordingly if this is not the case.\n",
    "    Pkg.activate(jenv)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "using AA228FinalProject\n",
    "using POMDPs\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using ParticleFilters\n",
    "using POMDPSimulators\n",
    "using Cairo\n",
    "using Gtk\n",
    "using Random\n",
    "using Printf\n",
    "using StaticArrays\n",
    "import POMDPs.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoombaPOMDP{Bumper,Bool}(Bumper(), RoombaMDP{DiscreteRoombaStateSpace,Array{RoombaAct,1}}\n",
       "  v_max: Float64 10.0\n",
       "  om_max: Float64 1.0\n",
       "  dt: Float64 0.5\n",
       "  contact_pen: Float64 -1.0\n",
       "  time_pen: Float64 -0.1\n",
       "  goal_reward: Float64 10.0\n",
       "  stairs_penalty: Float64 -10.0\n",
       "  config: Int64 1\n",
       "  sspace: DiscreteRoombaStateSpace\n",
       "  room: AA228FinalProject.Room\n",
       "  aspace: Array{RoombaAct}((15,))\n",
       "  _amap: Dict{RoombaAct,Int64}\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor = Bumper()\n",
    "config = 1# 1,2, or 3\n",
    "\n",
    "num_x_pts = 50\n",
    "num_y_pts = 50\n",
    "num_th_pts = 20\n",
    "sspace = DiscreteRoombaStateSpace(num_x_pts,num_y_pts,num_th_pts)\n",
    "aspace = vec(collect(RoombaAct(v, om) for v in [0,1,10], om in [-1,0,1]))\n",
    "\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config, sspace=sspace, aspace=aspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mdp (generic function with 2 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_particles = 2000\n",
    "resampler = BumperResampler(num_particles)\n",
    "\n",
    "spf = SimpleParticleFilter(m, resampler)\n",
    "\n",
    "v_noise_coefficient = 2.0\n",
    "om_noise_coefficient = 0.5\n",
    "\n",
    "belief_updater = RoombaParticleFilter(spf, v_noise_coefficient, om_noise_coefficient);\n",
    "\n",
    "mdp(e::RoombaMDP) = e\n",
    "mdp(e::RoombaPOMDP) = e.mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Point Based Value Iteration Policy Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "discountFactor = 0.9\n",
    "goal_x, goal_y = get_goal_xy(m)\n",
    "\n",
    "if(config == 1 || config == 3)\n",
    "    ang_to_goal = 0.0\n",
    "end\n",
    "if(config == 2)\n",
    "    ang_to_goal = -2.0\n",
    "end\n",
    "\n",
    "si = RoombaState(-1.5, -1.5, 1.0, 0.)\n",
    "sf = RoombaState(goal_x, goal_y, 1.0, 0.)\n",
    "\n",
    "action = (10.0, ang_to_goal)\n",
    "startingReward = rew(m, si, action, sf)\n",
    "firstAlphaVector = []\n",
    "\n",
    "for state in 1:length(POMDPs.states(m))\n",
    "    push!(firstAlphaVector, (1/(1-discountFactor)) * startingReward)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B = []\n",
    "dist = initialstate_distribution(m)\n",
    "b = initialize_belief(belief_updater, dist)\n",
    "\n",
    "for i = 1:100\n",
    "    push!(B, b)\n",
    "    b = resample(resampler, b, belief_updater.spf.rng)\n",
    "end\n",
    "\n",
    "gamma = Dict()\n",
    "gamma[action] = firstAlphaVector\n",
    "gamma = @time RandomizedPointBasedUpdate(B, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellohellohello194.012022 seconds (2.46 G allocations: 159.132 GiB, 22.50% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 15 entries:\n",
       "  [5.0, 0.0]   => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [3.0, 2.0]   => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [10.0, 1.0]  => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [10.0, 2.0]  => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [5.0, 2.0]   => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [3.0, -2.0]  => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [3.0, -1.0]  => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [10.0, -1.0] => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [5.0, 1.0]   => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [5.0, -2.0]  => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [5.0, -1.0]  => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [10.0, 0.0]  => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [3.0, 0.0]   => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [10.0, -2.0] => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…\n",
       "  [3.0, 1.0]   => Any[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.…"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma2 = Dict()\n",
    "gamma2[action] = firstAlphaVector\n",
    "\n",
    "B = []\n",
    "dist = initialstate_distribution(m)\n",
    "b = initialize_belief(belief_updater, dist)\n",
    "\n",
    "for i = 1:100\n",
    "    push!(B, b)\n",
    "    b = resample(resampler, b, belief_updater.spf.rng)\n",
    "end\n",
    "\n",
    "gamma2 = @time RandomizedPointBasedUpdate(B, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma3 = Dict()\n",
    "gamma3[action] = firstAlphaVector\n",
    "\n",
    "B = []\n",
    "dist = initialstate_distribution(m)\n",
    "b = initialize_belief(belief_updater, dist)\n",
    "\n",
    "for i = 1:100\n",
    "    push!(B, b)\n",
    "    b = resample(resampler, b, belief_updater.spf.rng)\n",
    "end\n",
    "\n",
    "gamma3 = @time RandomizedPointBasedUpdate(B, gamma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = []\n",
    "dist = initialstate_distribution(m)\n",
    "b = initialize_belief(belief_updater, dist)\n",
    "\n",
    "for i = 1:100\n",
    "    push!(B, b)\n",
    "    b = resample(resampler, b, belief_updater.spf.rng)\n",
    "end\n",
    "\n",
    "gamma4 = Dict()\n",
    "gamma4[action] = firstAlphaVector\n",
    "\n",
    "gamma4 = @time RandomizedPointBasedUpdate(B, gamma3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = []\n",
    "dist = initialstate_distribution(m)\n",
    "b = initialize_belief(belief_updater, dist)\n",
    "\n",
    "for i = 1:100\n",
    "    push!(B, b)\n",
    "    b = resample(resampler, b, belief_updater.spf.rng)\n",
    "end\n",
    "\n",
    "gamma5 = Dict()\n",
    "gamma5[action] = firstAlphaVector\n",
    "\n",
    "gamma5 = @time RandomizedPointBasedUpdate(B, gamma4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma10 = Dict()\n",
    "gamma10[action] = firstAlphaVector\n",
    "for i = 1:10\n",
    "    B = []\n",
    "    dist = initialstate_distribution(m)\n",
    "    b = initialize_belief(belief_updater, dist)\n",
    "\n",
    "    for i = 1:100\n",
    "        push!(B, b)\n",
    "        b = resample(resampler, b, belief_updater.spf.rng)\n",
    "    end\n",
    "    gamma10 = @time RandomizedPointBasedUpdate(B, gamma10)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma25 = Dict()\n",
    "gamma25[action] = firstAlphaVector\n",
    "for i = 1:25\n",
    "    B = []\n",
    "    dist = initialstate_distribution(m)\n",
    "    b = initialize_belief(belief_updater, dist)\n",
    "\n",
    "    for i = 1:100\n",
    "        push!(B, b)\n",
    "        b = resample(resampler, b, belief_updater.spf.rng)\n",
    "    end\n",
    "    gamma25 = @time RandomizedPointBasedUpdate(B, gamma25)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probabilityVector (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function probabilityVector(m, b)    \n",
    "    probabilitiesB = zeros(length(POMDPs.states(m)))   \n",
    "    ps = probdict(b)\n",
    "    for i in keys(ps)\n",
    "        if(i in (POMDPs.states(m)))\n",
    "            location = findall(x->x==i, POMDPs.states(m))[1]\n",
    "            probabilitiesB[location] = ps[i]\n",
    "        end \n",
    "    end\n",
    "    return probabilitiesB\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probdict (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function probdict(b::AbstractParticleBelief{S}) where {S}\n",
    "    probs = Dict{S, Float64}()\n",
    "    for (i,p) in enumerate(particles(b))\n",
    "        if haskey(probs, p)\n",
    "            probs[p] += weight(b, i)/weight_sum(b)\n",
    "        else\n",
    "            probs[p] = weight(b, i)/weight_sum(b)\n",
    "        end\n",
    "    end\n",
    "    return probs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedPointBasedUpdate (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function RandomizedPointBasedUpdate(B, gamma)\n",
    "    BP = B\n",
    "    gammaP = Dict()\n",
    "    while !(isempty(BP))\n",
    "        b = rand(BP)\n",
    "        filter!(e->e != b,BP)\n",
    "        alphaVectors = backupBelief(b, gamma)\n",
    "        probabilities = probabilityVector(m, b)\n",
    "        for action in aspace\n",
    "            if (action in keys(gammaP))\n",
    "                maxU = -1000\n",
    "                oldMaxAlpha = gammaP[action] \n",
    "                oldUtility = oldMaxAlpha'probabilities\n",
    "                newUtility = alphaVectors[action]'probabilities\n",
    "                if (newUtility >= oldUtility)\n",
    "                    gammaP[action] = alphaVectors[action]\n",
    "                else\n",
    "                    gammaP[action] = oldMaxAlpha\n",
    "                end\n",
    "            else\n",
    "                gammaP[action] = alphaVectors[action]\n",
    "            end\n",
    "        end  \n",
    "    end\n",
    "    return gammaP\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backupBelief (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backupBelief(b, gamma)\n",
    "    O = [0, 1]\n",
    "    alphaAO = Dict()\n",
    "    alphaVectors = Dict()\n",
    "    actionNumber = 1\n",
    "\n",
    "    for action in aspace\n",
    "        alphaStateAction = []\n",
    "        for observation in O\n",
    "            alphaAO[[action, observation + 1]] = zeros(length(POMDPs.states(m)))\n",
    "            bp = AA228FinalProject.update(belief_updater, b, action, observation)\n",
    "            maxTranspose = -1000\n",
    "            probability = probabilityVector(m, bp)\n",
    "            for a in keys(gamma)\n",
    "                t = gamma[a]'probability\n",
    "                if (t > maxTranspose)\n",
    "                    maxTranspose = t\n",
    "                    alphaAO[[action, observation + 1]] = gamma[a]\n",
    "                end\n",
    "            end\n",
    "        end \n",
    "        \n",
    "        s = mean(b)\n",
    "        e, gr, gwn, sr, swn, next_x, next_y, next_th, next_status = collisionStatus(s, action)\n",
    "        sp = RoombaState(next_x, next_y, next_th, next_status)\n",
    "\n",
    "        alpha = Dict()        \n",
    "        stateNumber = 1\n",
    "        for state in POMDPs.states(m)  \n",
    "            push!(alphaStateAction, AA228FinalProject.reward(m, s, action, sp) + discountFactor * sumNextStateObservation(s, action, alphaAO, stateNumber))\n",
    "            stateNumber += 1\n",
    "        end\n",
    "    \n",
    "        alphaVectors[action] = alphaStateAction\n",
    "        actionNumber += 1\n",
    "    end\n",
    "    return alphaVectors\n",
    "end     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sumNextStateObservation (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sumNextStateObservation(s, a, alphaAO, stateNumber)\n",
    "    sum = 0\n",
    "    e, gr, gwn, sr, swn, next_x, next_y, next_th, next_status, sp = collisionStatus(s, a)\n",
    "    \n",
    "    ##0 -- does not hit wall (false), 1 -- hits wall (true)\n",
    "    for o in [0, 1]\n",
    "        observationFunction = 0\n",
    "        if o == 0\n",
    "            if(!wall_contact(e, s))\n",
    "                observationFunction = 1\n",
    "                ob = 0\n",
    "            end\n",
    "        elseif o == 1\n",
    "            if(wall_contact(e, s))\n",
    "                observationFunction = 1\n",
    "            end\n",
    "        end\n",
    "        transitionFunction = 1 \n",
    "        sum += observationFunction * transitionFunction * alphaAO[[a, o + 1]][stateNumber]\n",
    "    end\n",
    "    return sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ToEnd <: Policy\n",
    "    ts::Int64 # to track the current time-step.\n",
    "end\n",
    "\n",
    "function POMDPs.action(p::ToEnd, b::ParticleCollection{RoombaState})\n",
    "    maxUtility = -1000\n",
    "    actionSelection = 0\n",
    "    probability = probabilityVector(m, b)\n",
    "    for action in keys(gamma2)\n",
    "        alpha = gamma[action]\n",
    "        transpose = alpha'probability\n",
    "        if (transpose > maxUtility)\n",
    "            maxUtility = transpose\n",
    "            actionSelection = action\n",
    "        end\n",
    "    end\n",
    "    return RoombaAct(actionSelection[1], actionSelection[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first seed the environment\n",
    "Random.seed!(2)\n",
    "\n",
    "# reset the policy\n",
    "p = ToEnd(0) # here, the argument sets the time-steps elapsed to 0\n",
    "\n",
    "# run the simulation\n",
    "c = @GtkCanvas()\n",
    "win = GtkWindow(c, \"Roomba Environment\", 600, 600)\n",
    "for (t, step) in enumerate(stepthrough(m, p, belief_updater, max_steps=100))\n",
    "    @guarded draw(c) do widget\n",
    "\n",
    "        # the following lines render the room, the particles, and the roomba\n",
    "        ctx = getgc(c)\n",
    "        set_source_rgb(ctx,1,1,1)\n",
    "        paint(ctx)\n",
    "        render(ctx, m, step)\n",
    "\n",
    "        # render some information that can help with debugging\n",
    "        # here, we render the time-step, the state, and the observation\n",
    "        move_to(ctx,300,400)\n",
    "        show_text(ctx, @sprintf(\"t=%d, state=%s, o=%.3f\",t,string(step.s),step.o))\n",
    "    end\n",
    "    show(c)\n",
    "    sleep(0.1) # to slow down the simulation\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Total Reward: -10.750, StdErr Total Reward: 0.224"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "for exp = 1:4\n",
    "    Random.seed!(exp)\n",
    "    p = ToEnd(0)\n",
    "    traj_rewards = sum([step.r for step in stepthrough(m,p,belief_updater, max_steps=100)])\n",
    "    push!(total_rewards, traj_rewards)\n",
    "end\n",
    "\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collisionStatus (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function collisionStatus(s, a)\n",
    "    e = mdp(m)\n",
    "    v, om = action\n",
    "    v = clamp(v, 0.0, e.v_max)\n",
    "    om = clamp(om, -e.om_max, e.om_max)\n",
    "\n",
    "    # propagate dynamics without wall considerations\n",
    "    x, y, th, _ = s\n",
    "    dt = e.dt\n",
    "\n",
    "    # dynamics assume robot rotates and then translates\n",
    "    next_th = wrap_to_pi(th + om*dt)\n",
    "\n",
    "    # make sure we arent going through a wall\n",
    "    p0 = SVector(x, y)\n",
    "    heading = SVector(cos(next_th), sin(next_th))\n",
    "    des_step = v*dt\n",
    "    next_x, next_y = AA228FinalProject.legal_translate(e.room, p0, heading, des_step)\n",
    "\n",
    "    # Determine whether goal state or stairs have been reached\n",
    "    grn = mdp(m).room.goal_rect\n",
    "    gwn = mdp(m).room.goal_wall\n",
    "    srn = mdp(m).room.stair_rect\n",
    "    swn = mdp(m).room.stair_wall\n",
    "    gr = mdp(m).room.rectangles[grn]\n",
    "    sr = mdp(m).room.rectangles[srn]    \n",
    "    \n",
    "    next_status = 1.0*AA228FinalProject.contact_wall(gr, gwn, [next_x, next_y]) - 1.0*AA228FinalProject.contact_wall(sr, swn, [next_x, next_y])\n",
    "    sp = RoombaState(next_x, next_y, next_th, next_status)\n",
    "    \n",
    "    return e, gr, gwn, sr, swn, next_x, next_y, next_th, next_status, sp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rew(m::RoombaPOMDP{Bumper,Bool}, s, a, sp)\n",
    "    # penalty for each timestep elapsed\n",
    "    cum_reward = mdp(m).time_pen\n",
    "\n",
    "    # penalty for bumping into wall (not incurred for consecutive contacts)\n",
    "    previous_wall_contact = wall_contact(m,s)\n",
    "    current_wall_contact = wall_contact(m,sp)\n",
    "    if(!previous_wall_contact && current_wall_contact)\n",
    "        cum_reward += mdp(m).contact_pen\n",
    "    end\n",
    "\n",
    "    # terminal rewards\n",
    "    cum_reward += mdp(m).goal_reward*(sp.status == 1.0)\n",
    "    cum_reward += mdp(m).stairs_penalty*(sp.status == -1.0)\n",
    "\n",
    "    return cum_reward  \n",
    "end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
