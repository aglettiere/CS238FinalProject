{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/anasnicholls/Desktop/CS238FinalProject/Project.toml\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate project environment\n",
    "# include these lines of code in any future scripts/notebooks\n",
    "#---\n",
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"AA228FinalProject\")\n",
    "    jenv = joinpath(dirname(@__FILE__()), \".\") # this assumes the notebook is in the same dir\n",
    "    # as the Project.toml file, which should be in top level dir of the project. \n",
    "    # Change accordingly if this is not the case.\n",
    "    Pkg.activate(jenv)\n",
    "end\n",
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/JuliaPOMDP`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaPOMDP/Registry`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/Desktop/CS238FinalProject/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/Desktop/CS238FinalProject/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "using AA228FinalProject\n",
    "using POMDPs\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using ParticleFilters\n",
    "using POMDPSimulators\n",
    "using Cairo\n",
    "using Gtk\n",
    "using Random\n",
    "using Printf\n",
    "using Pkg; Pkg.add(\"DiscreteValueIteration\")\n",
    "using DiscreteValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiscreteRoombaStateSpace(1.0204081632653061, 1.0204081632653061, 0.3306939635357677, [-30.0, 20.0], [-30.0, 20.0])\n",
      "RoombaAct[[0.0, -1.0], [1.0, -1.0], [10.0, -1.0], [0.0, 0.0], [1.0, 0.0], [10.0, 0.0], [0.0, 1.0], [1.0, 1.0], [10.0, 1.0]]\n",
      "INFO: POMDPs.jl requirements for \u001b[34msolve(::SparseValueIterationSolver, ::Union{MDP,POMDP})\u001b[39m and dependencies. ([✔] = implemented correctly; [X] = missing)\n",
      "\n",
      "For \u001b[34msolve(::SparseValueIterationSolver, ::Union{MDP,POMDP})\u001b[39m:\n",
      "\u001b[32m  [✔] discount(::RoombaMDP)\u001b[39m\n",
      "\u001b[32m  [✔] n_states(::RoombaMDP)\u001b[39m\n",
      "\u001b[32m  [✔] n_actions(::RoombaMDP)\u001b[39m\n",
      "\u001b[32m  [✔] transition(::RoombaMDP, ::RoombaState, ::RoombaAct)\u001b[39m\n",
      "\u001b[32m  [✔] reward(::RoombaMDP, ::RoombaState, ::RoombaAct, ::RoombaState)\u001b[39m\n",
      "\u001b[32m  [✔] stateindex(::RoombaMDP, ::RoombaState)\u001b[39m\n",
      "\u001b[32m  [✔] actionindex(::RoombaMDP, ::RoombaAct)\u001b[39m\n",
      "\u001b[32m  [✔] actions(::RoombaMDP, ::RoombaState)\u001b[39m\n",
      "\u001b[32m  [✔] support(::Deterministic)\u001b[39m\n",
      "\u001b[32m  [✔] pdf(::Deterministic, ::RoombaState)\u001b[39m\n",
      "For \u001b[34mordered_states(::Union{MDP,POMDP})\u001b[39m (in solve(::SparseValueIterationSolver, ::Union{MDP,POMDP})):\n",
      "\u001b[32m  [✔] states(::RoombaMDP)\u001b[39m\n",
      "For \u001b[34mordered_actions(::Union{MDP,POMDP})\u001b[39m (in solve(::SparseValueIterationSolver, ::Union{MDP,POMDP})):\n",
      "\u001b[32m  [✔] actions(::RoombaMDP)\u001b[39m\n",
      "RoombaMDP{DiscreteRoombaStateSpace,Array{RoombaAct,1}}\n",
      "  v_max: Float64 10.0\n",
      "  om_max: Float64 1.0\n",
      "  dt: Float64 0.5\n",
      "  contact_pen: Float64 -1.0\n",
      "  time_pen: Float64 -0.1\n",
      "  goal_reward: Float64 10.0\n",
      "  stairs_penalty: Float64 -10.0\n",
      "  config: Int64 3\n",
      "  sspace: DiscreteRoombaStateSpace\n",
      "  room: AA228FinalProject.Room\n",
      "  aspace: Array{RoombaAct}((9,))\n",
      "  _amap: Dict{RoombaAct,Int64}\n",
      "\n",
      "hi\n",
      "here\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sensor = Bumper()\n",
    "config = 3 # 1,2, or 3\n",
    "num_y_pts = 50\n",
    "num_x_pts = 50\n",
    "num_th_pts = 20\n",
    "sspace = DiscreteRoombaStateSpace(num_x_pts,num_y_pts,num_th_pts)\n",
    "println(sspace)\n",
    "aspace = vec(collect(RoombaAct(v, om) for v in [0,1,10], om in [-1,0,1]))\n",
    "println(aspace)\n",
    "@requirements_info SparseValueIterationSolver() RoombaMDP(config=config, sspace=sspace, aspace=aspace)\n",
    "mdp=RoombaMDP(config=config, sspace=sspace, aspace=aspace)\n",
    "println(mdp)\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=mdp)\n",
    "println(\"hi\")\n",
    "solver = SparseValueIterationSolver(max_iterations=100, belres=1e-3) # initializes the Solver type\n",
    "#solver = ValueIterationSolver(max_iterations=100, belres=1e-3) # initializes the Solver type\n",
    "println(\"here\")\n",
    "policy = solve(solver, mdp) # runs value iterations\n",
    "println(\"here\")\n",
    "#println(policy)\n",
    "# for elem in sspace\n",
    "#     a = action(policy, elem)\n",
    "#     println(a)\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler"
     ]
    }
   ],
   "source": [
    "num_particles = 2000\n",
    "resampler = BumperResampler(num_particles)\n",
    "\n",
    "spf = SimpleParticleFilter(mdp, resampler)\n",
    "\n",
    "v_noise_coefficient = 2.0\n",
    "om_noise_coefficient = 0.5\n",
    "print(\"sampler\")\n",
    "\n",
    "belief_updater = RoombaParticleFilter(spf, v_noise_coefficient, om_noise_coefficient);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Baseline Policy- Discretized Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy to test\n",
    "mutable struct ToEnd <: Policy\n",
    "    ts::Int64 # to track the current time-step.\n",
    "end\n",
    "\n",
    "# extract goal for heuristic controller\n",
    "goal_xy = get_goal_xy(m)\n",
    "\n",
    "# define a new function that takes in the policy struct and current belief,\n",
    "# and returns the desired action\n",
    "function POMDPs.action(p::ToEnd, b::ParticleCollection{RoombaState})\n",
    "    # compute mean belief of a subset of particles\n",
    "    #println(\"policy\")\n",
    "    println(\"here\")\n",
    "#     if p.ts < 10\n",
    "#         p.ts += 1\n",
    "#         return RoombaAct(0.,1.0) # all actions are of type RoombaAct(v,om)\n",
    "#     end\n",
    "#     p.ts += 1\n",
    "    \n",
    "#     v = 2.5\n",
    "    s = mean(b)\n",
    "    return action(policy, s)\n",
    "#     x,y,th = s[1:3]\n",
    "    \n",
    "#     ## If you hit a wall:\n",
    "#     if wall_contact(m, s)\n",
    "#         randomAngle = rand(r[1:3]) * pi*2\n",
    "#         return RoombaAct(0, randomAngle)\n",
    "#     else\n",
    "#         return RoombaAct(v, th)\n",
    "#     end\n",
    "\n",
    "#     return RoombaAct(v, th)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: missing comma or ) in argument list",
     "output_type": "error",
     "traceback": [
      "syntax: missing comma or ) in argument list",
      ""
     ]
    }
   ],
   "source": [
    "# first seed the environment\n",
    "Random.seed!(2)\n",
    "\n",
    "# reset the policy\n",
    "p = ToEnd(0) # here, the argument sets the time-steps elapsed to 0\n",
    "\n",
    "# run the simulation\n",
    "c = @GtkCanvas()\n",
    "win = GtkWindow(c, \"Roomba Environment\", 600, 600)\n",
    "#for (t, step) in enumerate(stepthrough(mdp, p, belief_updater, max_steps=100))\n",
    "for (t, step) in enumerate(stepthrough(mdp, policy max_steps=100))\n",
    "    @guarded draw(c) do widget\n",
    "        \n",
    "        # the following lines render the room, the particles, and the roomba\n",
    "        ctx = getgc(c)\n",
    "        set_source_rgb(ctx,1,1,1)\n",
    "        paint(ctx)\n",
    "        render(ctx, mdp, step)\n",
    "        \n",
    "        # render some information that can help with debugging\n",
    "        # here, we render the time-step, the state, and the observation\n",
    "        move_to(ctx,300,400)\n",
    "        show_text(ctx, @sprintf(\"t=%d, state=%s, o=%.3f\",t,string(step.s),step.o))\n",
    "    end\n",
    "    show(c)\n",
    "    sleep(0.1) # to slow down the simulation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Here, we demonstate a simple evaluation of the policy's performance for a few random seeds. This is meant to serve only as an example, and we encourage you to develop your own evaluation metrics.\n",
    "We intialize the robot using five different random seeds, and simulate its performance for 100 time-steps. We then sum the rewards experienced during its interaction with the environment and track this total reward for the five trials. Finally, we report the mean and standard error for the total reward. The standard error is the standard deviation of a sample set divided by the square root of the number of samples, and represents the uncertainty in the estimate of the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stepthrough\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Mean Total Reward: 8.300, StdErr Total Reward: 0.126"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "total_rewards = []\n",
    "println(\"stepthrough\")\n",
    "for exp = 1:5\n",
    "    println(string(exp))\n",
    "\n",
    "    Random.seed!(exp)\n",
    "    \n",
    "    p = ToEnd(0)\n",
    "    #print(step.r for step in stepthrough(m,p,belief_updater, max_steps=100))\n",
    "    traj_rewards = sum([step.r for step in stepthrough(mdp,policy, max_steps=100)])\n",
    "    #traj_rewards = sum([step.r for step in stepthrough(mdp,p,belief_updater, max_steps=100)])\n",
    "    \n",
    "    push!(total_rewards, traj_rewards)\n",
    "end\n",
    "\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
